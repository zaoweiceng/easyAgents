"""
æ–‡æœ¬æ–‡ä»¶è¯»å–Agent - è¯»å–å’Œåˆ†ææ–‡æœ¬æ–‡ä»¶

åŠŸèƒ½ï¼š
1. è¯»å–ç”¨æˆ·ä¸Šä¼ çš„æ–‡æœ¬æ–‡ä»¶ï¼ˆtxt, md, csv, json, xml, yaml, logç­‰ï¼‰
2. æ ¹æ®ç”¨æˆ·é—®é¢˜æ™ºèƒ½æå–ç›¸å…³éƒ¨åˆ†
3. å°†æå–çš„å†…å®¹äº¤ç»™åç»­Agentè¿›è¡Œæ€»ç»“å’Œåˆ†æ
"""

import re
import os
import json
import logging
from typing import Dict, Any, List, Optional
from core.agent import Agent
from core.base_model import Message
from core.prompt.template_model import PromptTemplate

logger = logging.getLogger(__name__)

# ================================
# æç¤ºè¯æ¨¡æ¿
# ================================

system_instructions = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡ä»¶å†…å®¹åˆ†æä¸“å®¶ï¼Œæ“…é•¿ä»æ–‡æœ¬æ–‡ä»¶ä¸­æå–å’Œæ•´ç†ä¿¡æ¯ã€‚

ä½ éœ€è¦èƒ½å¤Ÿï¼š
1. è¯†åˆ«ç”¨æˆ·ä¸Šä¼ çš„æ–‡æœ¬æ–‡ä»¶
2. ç†è§£ç”¨æˆ·çš„é—®é¢˜å’Œéœ€æ±‚
3. ä»æ–‡ä»¶ä¸­æå–ç›¸å…³çš„å†…å®¹ç‰‡æ®µ
4. å°†æå–çš„å†…å®¹æ•´ç†æˆæ˜“äºç†è§£çš„æ ¼å¼
"""

core_instructions = """
# ä»»åŠ¡æµç¨‹

1. åˆ†æç”¨æˆ·è¯·æ±‚ï¼š
   - è¯†åˆ«æ–‡ä»¶IDï¼ˆæ ¼å¼ï¼š[æ–‡ä»¶: filename.txt, ID: xxx]ï¼‰
   - ç†è§£ç”¨æˆ·çš„é—®é¢˜æˆ–éœ€æ±‚
   - ç¡®å®šéœ€è¦æå–çš„å†…å®¹ç±»å‹

2. è¯»å–å¹¶å¤„ç†æ–‡ä»¶ï¼š
   - æ ¹æ®æ–‡ä»¶IDè¯»å–æ–‡ä»¶å†…å®¹
   - æ ¹æ®ç”¨æˆ·é—®é¢˜æå–ç›¸å…³éƒ¨åˆ†
   - å¦‚æœç”¨æˆ·æ²¡æœ‰ç‰¹å®šé—®é¢˜ï¼Œè¿”å›æ–‡ä»¶å¼€å¤´éƒ¨åˆ†

3. è¿”å›ç»“æœï¼š
   - å°†æå–çš„å®Œæ•´å†…å®¹æ”¾åœ¨answerå­—æ®µä¸­
   - æä¾›æ–‡ä»¶çš„åŸºæœ¬ä¿¡æ¯ï¼ˆè¡Œæ•°ã€å­—ç¬¦æ•°ç­‰ï¼‰
   - è¿”å›æˆåŠŸçŠ¶æ€

# é‡è¦è¯´æ˜

- ä½ çš„ä»»åŠ¡æ˜¯è¯»å–å’Œæå–æ–‡ä»¶å†…å®¹ï¼Œä¸è¦è¿›è¡Œå¤æ‚çš„åˆ†æ
- å°†å®Œæ•´çš„æ–‡ä»¶å†…å®¹ä¼ é€’ç»™general_agentè¿›è¡Œæœ€ç»ˆæ€»ç»“
- å¦‚æœæ–‡ä»¶IDä¸å­˜åœ¨ï¼Œè¿”å›å‹å¥½çš„é”™è¯¯æç¤º
"""


class TextReaderAgent(Agent):
    """æ–‡æœ¬æ–‡ä»¶è¯»å–å’Œåˆ†æAgent"""

    def __init__(self):
        super().__init__(
            name="text_reader_agent",
            description="è¯»å–ç”¨æˆ·ä¸Šä¼ çš„æ–‡æœ¬æ–‡ä»¶ï¼Œæå–ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚æ”¯æŒtxtã€mdã€csvã€jsonã€xmlã€yamlã€logç­‰å¤šç§æ–‡æœ¬æ–‡ä»¶æ ¼å¼ã€‚",
            handles=[
                "è¯»å–æ–‡ä»¶", "åˆ†ææ–‡ä»¶", "æ–‡ä»¶å†…å®¹", "æŸ¥çœ‹æ–‡ä»¶", "æå–æ–‡ä»¶",
                "æ–‡ä»¶", "æ–‡æ¡£", "è¯»å–", "åˆ†æ", "æŸ¥çœ‹",
                "txt", "csv", "json", "xml", "yaml", "log",
                "[æ–‡ä»¶:", "ID:",  # æ–‡ä»¶IDæ ¼å¼
            ],
            parameters={
                "query": "ç”¨æˆ·çš„é—®é¢˜æˆ–éœ€æ±‚",
                "file_id": "ä¸Šä¼ æ–‡ä»¶çš„ID"
            }
        )

        # åˆå§‹åŒ–æç¤ºè¯æ¨¡æ¿
        self.prompt_template = PromptTemplate(
            system_instructions=system_instructions,
            available_agents=None,  # ç”±agent_manageråŠ¨æ€è®¾ç½®
            core_instructions=core_instructions,
            data_fields=None
        )

    def run(self, message: Message) -> Message:
        """ä¸»å¤„ç†é€»è¾‘"""
        # ä»æ¶ˆæ¯ä¸­æå–æ–‡ä»¶IDï¼ˆå‚è€ƒ web_summarizer_agent çš„åšæ³•ï¼‰
        file_id = self._extract_file_id(message)

        # éªŒè¯æ–‡ä»¶ID
        if not file_id:
            return Message(
                status="error",
                task_list=["è¯»å–æ–‡ä»¶"],
                data={"error": "æœªæ‰¾åˆ°æ–‡ä»¶ID"},
                next_agent="none",
                agent_selection_reason="æ–‡ä»¶IDæå–å¤±è´¥",
                message="è¯·åœ¨æ¶ˆæ¯ä¸­åŒ…å«æ–‡ä»¶IDï¼Œæ ¼å¼ï¼š[æ–‡ä»¶: filename.txt, ID: xxx]"
            )

        # è¯»å–æ–‡ä»¶
        file_data = self._read_text_file(file_id)

        if not file_data.get("success"):
            error_msg = file_data.get("error", "æ–‡ä»¶è¯»å–å¤±è´¥")
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {error_msg}")
            return Message(
                status="error",
                task_list=["è¯»å–æ–‡ä»¶"],
                data={"file_id": file_id, "error": error_msg},
                next_agent="none",
                agent_selection_reason="æ–‡ä»¶è¯»å–å¤±è´¥",
                message=f"æ— æ³•è¯»å–æ–‡ä»¶: {error_msg}"
            )

        # è·å–ç”¨æˆ·é—®é¢˜
        user_query = self._extract_user_query(message)

        # æ ¼å¼åŒ–æ–‡ä»¶å†…å®¹
        full_content = self._format_content_for_llm(
            file_data["content"],
            file_data["filename"],
            user_query
        )

        # ä¼ é€’ç»™ general_agent è¿›è¡Œåˆ†æ
        # æ„å»ºç®€çŸ­çš„æ‘˜è¦æ¶ˆæ¯
        file_summary = (f"å·²æˆåŠŸè¯»å–æ–‡ä»¶ {file_data['filename']}ï¼Œå…± {file_data['line_count']} è¡Œï¼Œ"
                       f"{file_data['char_count']} ä¸ªå­—ç¬¦ã€‚æ–‡ä»¶ç±»å‹: .{file_data['extension']}")

        # ä¸ºäº†é¿å…ä¸Šä¸‹æ–‡æº¢å‡ºï¼Œä¸ä¼ é€’å®Œæ•´çš„ raw_content
        # åªä¼ é€’æ ¼å¼åŒ–åçš„å†…å®¹ï¼ˆå·²ç»æˆªæ–­ï¼‰
        return Message(
            status="success",
            task_list=["è¯»å–æ–‡ä»¶", "æå–å†…å®¹"],
            data={
                "file_info": {
                    "filename": file_data["filename"],
                    "file_id": file_id,
                    "line_count": file_data["line_count"],
                    "char_count": file_data["char_count"],
                    "extension": file_data["extension"]
                },
                "answer": file_summary,
                "user_query": user_query,
                "formatted_content": full_content,  # å·²æˆªæ–­çš„æ ¼å¼åŒ–å†…å®¹
                # ä¸ä¼ é€’ raw_content ä»¥é¿å…ä¸Šä¸‹æ–‡æº¢å‡º
            },
            next_agent="general_agent",
            agent_selection_reason="æ–‡ä»¶å†…å®¹å·²è¯»å–å¹¶æ ¼å¼åŒ–ï¼Œä¼ é€’ç»™general_agentè¿›è¡Œåˆ†æå’Œå›ç­”",
            message=file_summary
        )

    def _extract_file_id(self, message: Message) -> Optional[str]:
        """ä»æ¶ˆæ¯ä¸­æå–æ–‡ä»¶IDï¼ˆå‚è€ƒ web_summarizer_agent çš„ URL æå–é€»è¾‘ï¼‰"""

        # ğŸ” è¯¦ç»†è°ƒè¯•ï¼šæ‰“å°å®Œæ•´çš„ message å¯¹è±¡
        logger.info(f"[text_reader_agent] ===== å¼€å§‹æå–æ–‡ä»¶ID =====")
        logger.info(f"[text_reader_agent] message.data: {message.data}")
        logger.info(f"[text_reader_agent] message.task_list: {message.task_list}")
        logger.info(f"[text_reader_agent] message.message: {getattr(message, 'message', 'N/A')}")

        # å°è¯•ä½¿ç”¨ model_dump() è·å–å®Œæ•´ç»“æ„
        try:
            message_dict = message.model_dump()
            logger.info(f"[text_reader_agent] message.model_dump(): {json.dumps(message_dict, ensure_ascii=False, indent=2)[:500]}")
        except:
            pass

        # 1. å…ˆå°è¯•ä» message.data è·å–
        if message.data:
            if isinstance(message.data, dict):
                logger.info(f"[text_reader_agent] message.data æ˜¯ dictï¼ŒåŒ…å«é”®: {list(message.data.keys())}")
                # æ£€æŸ¥å¸¸è§çš„å­—æ®µå
                for key in ["file_id", "fileId", "id", "content"]:
                    value = message.data.get(key)
                    logger.info(f"[text_reader_agent] æ£€æŸ¥å­—æ®µ '{key}': {value}")
                    if value:
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•æå–æ–‡ä»¶ID
                        if isinstance(value, str):
                            file_id = self._find_file_id_in_text(value)
                            if file_id:
                                logger.info(f"[text_reader_agent] âœ“ ä»å­—æ®µ '{key}' æå–åˆ°æ–‡ä»¶ID: {file_id}")
                                return file_id

        # 2. ä» message.message ä¸­æå–
        if hasattr(message, 'message') and message.message:
            logger.info(f"[text_reader_agent] ä» message.message ä¸­æŸ¥æ‰¾...")
            file_id = self._find_file_id_in_text(message.message)
            if file_id:
                logger.info(f"[text_reader_agent] âœ“ ä» message.message æå–åˆ°æ–‡ä»¶ID: {file_id}")
                return file_id

        # 3. ä» message.task_list ä¸­æå–
        if message.task_list:
            logger.info(f"[text_reader_agent] æ£€æŸ¥ task_list: {message.task_list}")
            for i, task in enumerate(message.task_list):
                logger.info(f"[text_reader_agent] æ£€æŸ¥ task_list[{i}]: {task}")
                if isinstance(task, str):
                    file_id = self._find_file_id_in_text(task)
                    if file_id:
                        logger.info(f"[text_reader_agent] âœ“ ä» task_list[{i}] æå–åˆ°æ–‡ä»¶ID: {file_id}")
                        return file_id

        # 4. æœ€åçš„å°è¯•ï¼šä»å®Œæ•´çš„ model_dump JSON ä¸­æŸ¥æ‰¾
        try:
            message_json = json.dumps(message.model_dump(), ensure_ascii=False)
            logger.info(f"[text_reader_agent] ä»å®Œæ•´ JSON ä¸­æŸ¥æ‰¾...")
            file_id = self._find_file_id_in_text(message_json)
            if file_id:
                logger.info(f"[text_reader_agent] âœ“ ä»å®Œæ•´ JSON æå–åˆ°æ–‡ä»¶ID: {file_id}")
                return file_id
        except:
            pass

        logger.warning(f"[text_reader_agent] âœ— æœªèƒ½ä»æ¶ˆæ¯ä¸­æå–æ–‡ä»¶ID")
        logger.info(f"[text_reader_agent] ===== æå–ç»“æŸ =====")
        return None

    def _find_file_id_in_text(self, text: str) -> Optional[str]:
        """ä»æ–‡æœ¬ä¸­æŸ¥æ‰¾æ–‡ä»¶ID"""
        # æ”¯æŒçš„æ ¼å¼ï¼š
        # [æ–‡ä»¶: name.txt, ID: uuid]
        # ID: uuid
        # file_id: uuid
        patterns = [
            r'\[æ–‡ä»¶:\s*[^,\]]+,\s*ID:\s*([a-f0-9-]+)\]',  # [æ–‡ä»¶: name, ID: uuid]
            r'ID:\s*([a-f0-9-]+)',  # ID: uuid
            r'file[_-]?id:\s*([a-f0-9-]+)',  # file_id: uuid (å¿½ç•¥å¤§å°å†™)
            r'"file_id"\s*:\s*"([a-f0-9-]+)"',  # JSONæ ¼å¼
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                file_id = match.group(1)
                logger.info(f"[text_reader_agent] æ‰¾åˆ°æ–‡ä»¶ID: {file_id}")
                return file_id

        return None

    def _extract_user_query(self, message: Message) -> str:
        """ä»æ¶ˆæ¯ä¸­æå–ç”¨æˆ·çš„é—®é¢˜"""
        # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„æ–‡æœ¬
        query_parts = []

        # ä» message.data è·å–
        if message.data and isinstance(message.data, dict):
            for key in ["content", "query", "user_query", "question"]:
                value = message.data.get(key)
                if value and isinstance(value, str):
                    query_parts.append(value)

        # ä» message.message è·å–
        if hasattr(message, 'message') and message.message:
            query_parts.append(message.message)

        # ä» task_list è·å–
        if message.task_list:
            query_parts.extend([str(t) for t in message.task_list])

        # åˆå¹¶å¹¶ç§»é™¤æ–‡ä»¶å¼•ç”¨
        combined = " ".join(query_parts)
        # ç§»é™¤æ–‡ä»¶å¼•ç”¨
        combined = re.sub(r'\[æ–‡ä»¶:\s*[^,\]]+,\s*ID:\s*[a-f0-9-]+\]', '', combined)
        combined = combined.strip()

        return combined if combined else "è¯·æ€»ç»“è¿™ä¸ªæ–‡ä»¶çš„å†…å®¹"

    def _read_text_file(self, file_id: str) -> Dict[str, Any]:
        """è¯»å–æ–‡æœ¬æ–‡ä»¶"""
        from core.file_service import get_file_service

        # ä½¿ç”¨æ–‡ä»¶æœåŠ¡æŸ¥æ‰¾æ–‡ä»¶è®°å½•
        file_service = get_file_service()
        file_record = file_service.get_file(file_id)

        if not file_record:
            # å¦‚æœç›´æ¥é€šè¿‡file_idæ‰¾ä¸åˆ°ï¼Œå°è¯•éå†ç›®å½•
            logger.warning(f"[text_reader_agent] æ— æ³•é€šè¿‡file_idæ‰¾åˆ°è®°å½•ï¼Œå°è¯•éå†ç›®å½•")
            return self._read_text_file_by_scan(file_id)

        # è·å–æ–‡ä»¶è·¯å¾„
        file_path = file_service.get_file_path(file_id)
        if not file_path:
            return {
                "success": False,
                "error": "æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨"
            }

        filename = file_record.original_filename
        ext = filename.rsplit('.', 1)[-1].lower() if '.' in filename else ''

        # æ”¯æŒçš„æ–‡æœ¬æ–‡ä»¶ç±»å‹
        text_extensions = {
            'txt', 'md', 'markdown', 'csv', 'json', 'xml', 'yaml', 'yml',
            'log', 'conf', 'ini', 'env', 'py', 'js', 'ts', 'jsx', 'tsx',
            'java', 'c', 'cpp', 'h', 'go', 'rs', 'sql', 'html', 'css'
        }

        if ext not in text_extensions:
            return {
                "success": False,
                "error": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: .{ext}"
            }

        # è¯»å–æ–‡ä»¶
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            lines = content.split('\n')

            logger.info(f"[text_reader_agent] âœ“ æˆåŠŸè¯»å–æ–‡ä»¶: {filename}")

            return {
                "success": True,
                "filename": filename,
                "file_id": file_id,
                "content": content,
                "line_count": len(lines),
                "char_count": len(content),
                "extension": ext
            }

        except UnicodeDecodeError:
            # å°è¯•GBKç¼–ç 
            try:
                with open(file_path, 'r', encoding='gbk') as f:
                    content = f.read()

                lines = content.split('\n')

                return {
                    "success": True,
                    "filename": filename,
                    "file_id": file_id,
                    "content": content,
                    "line_count": len(lines),
                    "char_count": len(content),
                    "extension": ext
                }
            except Exception as e:
                return {
                    "success": False,
                    "error": f"æ–‡ä»¶ç¼–ç é”™è¯¯: {str(e)}"
                }

        except Exception as e:
            return {
                "success": False,
                "error": f"è¯»å–å¼‚å¸¸: {str(e)}"
            }

    def _read_text_file_by_scan(self, file_id: str) -> Dict[str, Any]:
        """é€šè¿‡éå†ç›®å½•æŸ¥æ‰¾æ–‡ä»¶ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        storage_dir = "data/files/uploads"

        if not os.path.exists(storage_dir):
            return {
                "success": False,
                "error": "ä¸Šä¼ ç›®å½•ä¸å­˜åœ¨"
            }

        # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶ï¼ŒæŸ¥æ‰¾åŒ¹é…çš„
        all_files = []
        for filename in os.listdir(storage_dir):
            filepath = os.path.join(storage_dir, filename)
            try:
                # è·å–æ–‡ä»¶çš„å®Œæ•´ä¿¡æ¯ï¼ˆè¯»å–æ–‡ä»¶çš„å‰å‡ ä¸ªå­—èŠ‚ï¼‰
                with open(filepath, 'rb') as f:
                    header = f.read(100)
                    # å°è¯•è§£ææ˜¯å¦æœ‰æ–‡ä»¶IDä¿¡æ¯
                    # è¿™é‡Œç®€å•å¤„ç†ï¼šè¿”å›æ‰€æœ‰æ–‡ä»¶åˆ—è¡¨ä¾›è°ƒè¯•
                    all_files.append(filename)
            except:
                pass

        logger.warning(f"[text_reader_agent] ç›®å½•ä¸­çš„æ–‡ä»¶: {all_files}")
        return {
            "success": False,
            "error": f"æ–‡ä»¶ID {file_id} ä¸å­˜åœ¨ï¼Œç›®å½•ä¸­çš„æ–‡ä»¶: {all_files}"
        }

    def _format_content_for_llm(
        self,
        content: str,
        filename: str,
        user_query: str
    ) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å†…å®¹ä¾›LLMä½¿ç”¨ï¼ˆæ™ºèƒ½åˆ†æ®µå’Œæ£€ç´¢ï¼‰"""
        lines = content.split('\n')

        parts = []
        parts.append(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        parts.append(f"æ–‡ä»¶å: {filename}\n")
        parts.append(f"æ€»è¡Œæ•°: {len(lines)}\n")
        parts.append(f"å­—ç¬¦æ•°: {len(content)}\n")
        parts.append(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        max_safe_chars = 2500  # å®‰å…¨å­—ç¬¦æ•°ï¼Œçº¦ 1200-1500 tokens

        if len(content) <= max_safe_chars:
            # æ–‡ä»¶è¾ƒå°ï¼Œç›´æ¥è¿”å›å®Œæ•´å†…å®¹
            parts.append("ã€å®Œæ•´æ–‡ä»¶å†…å®¹ã€‘\n")
            parts.append(content)
        else:
            # æ–‡ä»¶è¾ƒå¤§ï¼Œä½¿ç”¨æ™ºèƒ½åˆ†æ®µæ£€ç´¢
            parts.append(f"âš ï¸ æ–‡ä»¶è¾ƒå¤§ ({len(content)} å­—ç¬¦)ï¼Œå·²æ ¹æ®é—®é¢˜æ™ºèƒ½æ£€ç´¢ç›¸å…³ç‰‡æ®µ\n")
            parts.append(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")

            # æ£€æŸ¥æ˜¯å¦æ˜¯ä»£ç æ–‡ä»¶
            is_code_file = any(filename.endswith(ext) for ext in ['.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.go', '.rs'])

            if is_code_file:
                # ä»£ç æ–‡ä»¶ï¼šæå–ç»“æ„
                relevant_content = self._extract_relevant_code_sections(content, filename, user_query)
            else:
                # æ–‡æœ¬æ–‡ä»¶ï¼šåˆ†æ®µæ£€ç´¢
                relevant_content = self._search_relevant_chunks(content, user_query)

            parts.append("ã€æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ã€‘\n")
            parts.append(relevant_content)

            parts.append(f"\n{'='*50}\n")
            parts.append(f"æç¤ºï¼šä»¥ä¸Šæ˜¯æ ¹æ®æ‚¨çš„é—®é¢˜æ£€ç´¢åˆ°çš„æœ€ç›¸å…³ç‰‡æ®µã€‚\n")
            parts.append(f"å¦‚æœéœ€è¦æŸ¥çœ‹å…¶ä»–éƒ¨åˆ†ï¼Œè¯·æå‡ºæ›´å…·ä½“çš„é—®é¢˜ã€‚\n")
            parts.append(f"{'='*50}\n")

        # æ·»åŠ ç”¨æˆ·é—®é¢˜
        if user_query and user_query != "è¯·æ€»ç»“è¿™ä¸ªæ–‡ä»¶çš„å†…å®¹":
            parts.append(f"\n{'='*50}\n")
            parts.append(f"ã€ç”¨æˆ·é—®é¢˜ã€‘\n{user_query}\n")
            parts.append(f"{'='*50}\n")

        return "\n".join(parts)

    def _split_into_chunks(self, text: str, chunk_size: int = 500, overlap: int = 50) -> list:
        """
        å°†æ–‡æœ¬åˆ†å‰²æˆé‡å çš„å—

        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬
            chunk_size: æ¯å—çš„å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            overlap: å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°

        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        chunks = []
        start = 0
        text_length = len(text)

        while start < text_length:
            end = start + chunk_size

            # å°½é‡åœ¨æ¢è¡Œç¬¦å¤„åˆ†å‰²ï¼Œé¿å…æˆªæ–­å¥å­
            if end < text_length:
                # æŸ¥æ‰¾æœ€è¿‘çš„æ¢è¡Œç¬¦
                newline_pos = text.rfind('\n', start, end)
                if newline_pos > start + chunk_size // 2:  # ç¡®ä¿ä¸ä¼šå¤ªå°
                    end = newline_pos + 1

            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)

            # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªå—ï¼Œä¿ç•™é‡å éƒ¨åˆ†
            start = end - overlap if end < text_length else text_length

        return chunks

    def _extract_keywords(self, query: str) -> list:
        """
        ä»ç”¨æˆ·é—®é¢˜ä¸­æå–å…³é”®è¯

        Args:
            query: ç”¨æˆ·é—®é¢˜

        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        import re

        # ç§»é™¤å¸¸è§çš„æ— æ„ä¹‰è¯
        stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'å¦‚æœ', 'é‚£ä¹ˆ',
                     'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'å“ªäº›', 'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™äº›',
                     'the', 'is', 'a', 'an', 'and', 'or', 'but', 'if', 'then', 'what',
                     'how', 'why', 'which', 'this', 'that', 'these', 'those', 'file',
                     'æ–‡ä»¶', 'å†…å®¹', 'ä½œç”¨', 'åŠŸèƒ½', 'ç”¨äº', 'åšä»€ä¹ˆ'}

        # æå–ä¸­æ–‡å’Œè‹±æ–‡å•è¯
        words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z0-9_]+', query.lower())

        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        keywords = [w for w in words if w not in stop_words and len(w) > 1]

        return keywords

    def _calculate_chunk_relevance(self, chunk: str, keywords: list) -> float:
        """
        è®¡ç®—æ–‡æœ¬å—ä¸å…³é”®è¯çš„ç›¸å…³æ€§å¾—åˆ†

        Args:
            chunk: æ–‡æœ¬å—
            keywords: å…³é”®è¯åˆ—è¡¨

        Returns:
            ç›¸å…³æ€§å¾—åˆ†
        """
        if not keywords:
            return 0.5  # æ²¡æœ‰å…³é”®è¯æ—¶è¿”å›ä¸­ç­‰å¾—åˆ†

        chunk_lower = chunk.lower()
        score = 0.0

        for keyword in keywords:
            keyword_lower = keyword.lower()
            # ç²¾ç¡®åŒ¹é…å¾—é«˜åˆ†
            count = chunk_lower.count(keyword_lower)
            score += count * 2

            # è¯é¦–åŒ¹é…ï¼ˆå¦‚ "def" åŒ¹é… "define"ï¼‰
            if keyword_lower in chunk_lower:
                score += 1

        # æ ‡å‡†åŒ–å¾—åˆ†
        return min(score / len(keywords), 10.0)

    def _search_relevant_chunks(self, content: str, query: str) -> str:
        """
        æ ¹æ®ç”¨æˆ·é—®é¢˜æœç´¢æœ€ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µ

        Args:
            content: æ–‡ä»¶å†…å®¹
            query: ç”¨æˆ·é—®é¢˜

        Returns:
            ç»„åˆçš„ç›¸å…³ç‰‡æ®µ
        """
        # æå–å…³é”®è¯
        keywords = self._extract_keywords(query)

        # åˆ†å‰²æˆå—
        chunks = self._split_into_chunks(content, chunk_size=600, overlap=100)

        if not chunks:
            return content[:2500]  # é™çº§æ–¹æ¡ˆ

        # è®¡ç®—æ¯ä¸ªå—çš„ç›¸å…³æ€§
        chunk_scores = []
        for i, chunk in enumerate(chunks):
            score = self._calculate_chunk_relevance(chunk, keywords)
            chunk_scores.append((i, chunk, score))

        # æŒ‰ç›¸å…³æ€§æ’åº
        chunk_scores.sort(key=lambda x: x[2], reverse=True)

        # é€‰æ‹©æœ€ç›¸å…³çš„å‡ ä¸ªå—ï¼ˆæ§åˆ¶æ€»é•¿åº¦ï¼‰
        selected_chunks = []
        total_chars = 0
        max_chars = 2000

        for i, chunk, score in chunk_scores:
            if total_chars + len(chunk) > max_chars:
                break
            selected_chunks.append((i, chunk, score))
            total_chars += len(chunk)

        # æŒ‰åŸå§‹é¡ºåºé‡æ–°æ’åˆ—
        selected_chunks.sort(key=lambda x: x[0])

        # ç»„åˆç»“æœ
        result_parts = []
        for i, chunk, score in selected_chunks:
            result_parts.append(f"ã€ç‰‡æ®µ {i+1}ã€‘(ç›¸å…³åº¦: {score:.1f})\n{chunk}\n")

        return '\n'.join(result_parts) if result_parts else content[:2500]

    def _extract_relevant_code_sections(self, content: str, filename: str, query: str) -> str:
        """
        ä»ä»£ç æ–‡ä»¶ä¸­æå–ä¸é—®é¢˜ç›¸å…³çš„éƒ¨åˆ†

        Args:
            content: ä»£ç å†…å®¹
            filename: æ–‡ä»¶å
            query: ç”¨æˆ·é—®é¢˜

        Returns:
            ç›¸å…³çš„ä»£ç æ®µ
        """
        lines = content.split('\n')

        # æå–å…³é”®è¯
        keywords = self._extract_keywords(query)

        # æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šå…³é”®å­—
        ext = filename.rsplit('.', 1)[-1].lower() if '.' in filename else ''

        # å®šä¹‰ä»£ç ç»“æ„å…³é”®å­—
        structure_keywords = {
            '.py': ['def ', 'class ', 'import ', 'from '],
            '.js': ['function ', 'class ', 'const ', 'let ', 'var ', 'import '],
            '.ts': ['function ', 'class ', 'const ', 'let ', 'var ', 'import ', 'interface ', 'type '],
            '.jsx': ['function ', 'class ', 'const ', 'let ', 'import '],
            '.tsx': ['function ', 'class ', 'const ', 'let ', 'import ', 'interface '],
            '.java': ['public ', 'private ', 'protected ', 'class ', 'interface '],
            '.go': ['func ', 'type ', 'import ', 'package '],
            '.rs': ['fn ', 'struct ', 'enum ', 'impl ', 'use ', 'mod '],
        }

        search_patterns = structure_keywords.get(f'.{ext}', [])

        # æå–ä»£ç æ®µï¼ˆæ¯ä¸ªå‡½æ•°/ç±»ä½œä¸ºä¸€ä¸ªæ®µï¼‰
        code_sections = []
        current_section = []
        section_start_line = 0
        indent_level = 0

        for i, line in enumerate(lines):
            stripped = line.strip()

            # æ£€æµ‹æ–°çš„ä»£ç æ®µï¼ˆå‡½æ•°/ç±»å®šä¹‰ï¼‰
            if any(pattern in stripped for pattern in search_patterns):
                # ä¿å­˜å‰ä¸€ä¸ªæ®µ
                if current_section:
                    section_text = '\n'.join(current_section)
                    score = self._calculate_chunk_relevance(section_text, keywords)
                    code_sections.append({
                        'start': section_start_line,
                        'end': i - 1,
                        'content': section_text,
                        'score': score
                    })

                # å¼€å§‹æ–°æ®µ
                current_section = [line]
                section_start_line = i
            elif current_section:
                # ç»§ç»­å½“å‰æ®µ
                current_section.append(line)

        # ä¿å­˜æœ€åä¸€ä¸ªæ®µ
        if current_section:
            section_text = '\n'.join(current_section)
            score = self._calculate_chunk_relevance(section_text, keywords)
            code_sections.append({
                'start': section_start_line,
                'end': len(lines) - 1,
                'content': section_text,
                'score': score
            })

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æ„åŒ–æ®µï¼Œé™çº§åˆ°è¡Œçº§åˆ«æ£€ç´¢
        if not code_sections:
            return self._search_relevant_chunks(content, query)

        # æŒ‰ç›¸å…³æ€§æ’åº
        code_sections.sort(key=lambda x: x['score'], reverse=True)

        # é€‰æ‹©æœ€ç›¸å…³çš„æ®µ
        selected_sections = []
        total_chars = 0
        max_chars = 2000

        for section in code_sections:
            if total_chars + len(section['content']) > max_chars:
                break
            selected_sections.append(section)
            total_chars += len(section['content'])

        # æŒ‰åŸå§‹é¡ºåºé‡æ–°æ’åˆ—
        selected_sections.sort(key=lambda x: x['start'])

        # ç»„åˆç»“æœ
        result_parts = []
        for section in selected_sections:
            result_parts.append(f"ã€ä»£ç æ®µ è¡Œ{section['start']+1}-{section['end']+1}ã€‘(ç›¸å…³åº¦: {section['score']:.1f})")
            result_parts.append(section['content'])
            result_parts.append("")

        return '\n'.join(result_parts) if selected_sections else content[:2500]


# å¯¼å‡ºAgentå®ä¾‹
text_reader_agent = TextReaderAgent()
